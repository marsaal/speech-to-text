from tokenizers import Tokenizer
from tokenizers.models import WordLevel
from tokenizers.pre_tokenizers import Split
from transformers import PreTrainedTokenizerFast
import os

# üî§ Vocabulari de car√†cters catalans complet
vocab_list = list(
    "abcdefghijklmnopqrstuvwxyz√ß√†√®√©√≠√Ø√≤√≥√∫√º" +
    "ABCDEFGHIJKLMNOPQRSTUVWXYZ√á√Ä√à√â√ç√è√í√ì√ö√ú" +
    " ,.;¬∑'-?!\""
)
vocab_list += ["<ctc_blank>", "<pad>", "<unk>"]

# Diccionari {car√†cter: ID}
vocab_dict = {c: i for i, c in enumerate(vocab_list)}

# Tokenizer WordLevel per car√†cter
tokenizer = Tokenizer(WordLevel(vocab=vocab_dict, unk_token="<unk>"))
tokenizer.pre_tokenizer = Split(pattern="", behavior="isolated")

# HuggingFace wrapper
hf_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    unk_token="<unk>",
    pad_token="<pad>"
)

# Desa
save_dir = "tokenizers/ctc_catalan_char_tokenizer"
os.makedirs(save_dir, exist_ok=True)
hf_tokenizer.save_pretrained(save_dir)

# Test
text = "L'√∫s d'algoritmes al sector p√∫blic."
encoded = hf_tokenizer(text)
decoded = ''.join(hf_tokenizer.convert_ids_to_tokens(encoded.input_ids))

print("Token IDs:", encoded.input_ids)
print("Decoded:", decoded)

# Mostra l'ID real del blank
blank_id = hf_tokenizer.convert_tokens_to_ids("<ctc_blank>")
print("CTC blank ID:", blank_id)
